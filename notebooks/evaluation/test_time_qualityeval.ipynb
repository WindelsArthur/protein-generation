{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cd7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniforge3/envs/eval_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats\n",
    "import faiss #type: ignore\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from typing import Union, Tuple\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Scientific computing\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.linalg import eigvals, eig\n",
    "from scipy.stats import entropy\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from top_pr import compute_top_pr as TopPR #type: ignore\n",
    "\n",
    "# Hugging Face transformers\n",
    "from transformers import EsmTokenizer, EsmForMaskedLM, EsmForProteinFolding #type: ignore\n",
    "\n",
    "# External soft alignment\n",
    "import sys, pathlib, os\n",
    "project_root = pathlib.Path.home() / \"projets\" / \"protein-generation\"\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from external.protein_embed_softalign.soft_align import soft_align\n",
    "\n",
    "def load_perplexity_model(\n",
    "    ppl_model_name: str, \n",
    "    device: str = \"cuda\"\n",
    ") -> Tuple[EsmForMaskedLM, EsmTokenizer]:\n",
    "    print(\"Loading perplexity model...\")\n",
    "    \n",
    "    # Load tokenizer and model from Hugging Face\n",
    "    ppl_tokenizer = EsmTokenizer.from_pretrained(ppl_model_name)\n",
    "    ppl_model = EsmForMaskedLM.from_pretrained(ppl_model_name)\n",
    "    \n",
    "    # Set model to evaluation mode and move to specified device\n",
    "    ppl_model.eval()\n",
    "    ppl_model.to(device)\n",
    "    \n",
    "    print(\"✓ Perplexity model loaded\")\n",
    "    return ppl_model, ppl_tokenizer\n",
    "\n",
    "\n",
    "def load_folding_model(\n",
    "    fold_model_name: str, \n",
    "    device: str\n",
    ") -> Tuple[EsmForProteinFolding, EsmTokenizer]:\n",
    "    print(\"Loading folding model...\")\n",
    "    \n",
    "    # Load tokenizer and folding model from Hugging Face\n",
    "    fold_tokenizer = EsmTokenizer.from_pretrained(fold_model_name)\n",
    "    fold_model = EsmForProteinFolding.from_pretrained(fold_model_name)\n",
    "    \n",
    "    # Set model to evaluation mode and move to specified device\n",
    "    fold_model.eval()\n",
    "    fold_model.to(device)\n",
    "    \n",
    "    print(\"✓ Folding model loaded\")\n",
    "    return fold_model, fold_tokenizer\n",
    "\n",
    "\n",
    "def get_sequence_embeddings(\n",
    "    sequences: List[str],\n",
    "    ppl_model: EsmForMaskedLM,\n",
    "    ppl_tokenizer: EsmTokenizer,\n",
    "    device: Union[str, torch.device],\n",
    "    show_progress: bool = True\n",
    ") -> np.ndarray:\n",
    "    embeddings = []\n",
    "    \n",
    "    # Setup iterator with or without progress bar\n",
    "    iterator = tqdm(sequences, desc=\"Extracting embeddings\") if show_progress else sequences\n",
    "    \n",
    "    for seq in iterator:\n",
    "        # Tokenize the protein sequence\n",
    "        inputs = ppl_tokenizer(seq, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get model outputs with hidden states from all layers\n",
    "            outputs = ppl_model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "            # Extract layer 6 hidden states (0-indexed, so layer 6 is index 6)\n",
    "            # Shape: [batch_size, seq_len, hidden_dim]\n",
    "            hidden_states = outputs.hidden_states[6]\n",
    "            \n",
    "            # Get actual sequence length (excluding special tokens)\n",
    "            seq_len = len(seq)\n",
    "            \n",
    "            # Extract embeddings for the actual sequence (excluding [CLS] and [SEP] tokens)\n",
    "            # [CLS] is at position 0, sequence starts at position 1\n",
    "            seq_embedding = hidden_states[0, 1:seq_len+1].mean(dim=0).cpu().numpy()\n",
    "            \n",
    "            embeddings.append(seq_embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def calculate_plddt(seq: str, fold_model, fold_tokenizer, device=\"cuda\") -> float:\n",
    "    inputs = fold_tokenizer(seq, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        plddt = fold_model(**inputs).plddt[0].mul_(100).mean()\n",
    "    \n",
    "    return float(plddt)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_perplexity(\n",
    "    seq: str,\n",
    "    ppl_model: EsmForMaskedLM,\n",
    "    ppl_tokenizer: EsmTokenizer,\n",
    "    device: Union[torch.device, str] = \"cuda\"\n",
    ") -> float:\n",
    "    # Tokenize sequence\n",
    "    inputs = ppl_tokenizer(seq, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get sequence length (excluding special tokens)\n",
    "    seq_len = len(seq)\n",
    "    \n",
    "    # Store original tokens for positions we'll mask\n",
    "    original_input_ids = inputs['input_ids'].clone()\n",
    "    \n",
    "    # Create batch with all positions masked (one sample per position)\n",
    "    batch_size = seq_len\n",
    "    batch_input_ids = original_input_ids.repeat(batch_size, 1)\n",
    "    batch_attention_mask = inputs['attention_mask'].repeat(batch_size, 1)\n",
    "    \n",
    "    # Mask each position in its corresponding batch sample\n",
    "    for i in range(batch_size):\n",
    "        batch_input_ids[i, i + 1] = ppl_tokenizer.mask_token_id  # +1 to skip [CLS]\n",
    "    \n",
    "    batch_inputs = {\n",
    "        'input_ids': batch_input_ids,\n",
    "        'attention_mask': batch_attention_mask\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Single forward pass for all masked positions\n",
    "        outputs = ppl_model(**batch_inputs)\n",
    "        logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Calculate loss for each position\n",
    "    for i in range(batch_size):\n",
    "        # Get the original token at position i+1 (skip [CLS])\n",
    "        original_token = original_input_ids[0, i + 1].item()\n",
    "        \n",
    "        # Get logits for the masked position in sample i\n",
    "        position_logits = logits[i, i + 1]  # +1 to skip [CLS]\n",
    "        \n",
    "        # Calculate log probability\n",
    "        log_probs = torch.nn.functional.log_softmax(position_logits, dim=-1)\n",
    "        token_loss = -log_probs[original_token].item()\n",
    "        total_loss += token_loss\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / seq_len\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def calculate_perplexity_simple(\n",
    "    seq: str,\n",
    "    ppl_model: EsmForMaskedLM,\n",
    "    ppl_tokenizer: EsmTokenizer,\n",
    "    device: Union[torch.device, str] = \"cuda\"\n",
    ") -> float:\n",
    "    inputs = ppl_tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ppl_model(**inputs, labels=inputs['input_ids'])\n",
    "        # La loss est déjà moyennée par token\n",
    "        perplexity = math.exp(outputs.loss.item())\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f736f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Loading perplexity model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Perplexity model loaded\n",
      "Loading folding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esmfold_v1 were not used when initializing EsmForProteinFolding: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForProteinFolding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForProteinFolding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Folding model loaded\n",
      "\n",
      "Séquence 1 (267 AA):\n",
      "  pLDDT: 79.65 (6.294s)\n",
      "  Perplexity: 10.37 (0.265s)\n",
      "  Simple Perplexity: 1.48 (0.010s)\n",
      "\n",
      "Séquence 2 (224 AA):\n",
      "  pLDDT: 69.86 (4.059s)\n",
      "  Perplexity: 17.38 (0.172s)\n",
      "  Simple Perplexity: 1.66 (0.006s)\n",
      "\n",
      "Séquence 3 (299 AA):\n",
      "  pLDDT: 75.57 (8.057s)\n",
      "  Perplexity: 9.76 (0.340s)\n",
      "  Simple Perplexity: 1.50 (0.007s)\n",
      "\n",
      "Séquence 4 (216 AA):\n",
      "  pLDDT: 87.33 (3.795s)\n",
      "  Perplexity: 9.42 (0.160s)\n",
      "  Simple Perplexity: 1.47 (0.007s)\n",
      "\n",
      "Séquence 5 (298 AA):\n",
      "  pLDDT: 83.23 (8.127s)\n",
      "  Perplexity: 7.29 (0.340s)\n",
      "  Simple Perplexity: 1.48 (0.007s)\n",
      "\n",
      "Séquence 6 (281 AA):\n",
      "  pLDDT: 52.47 (7.097s)\n",
      "  Perplexity: 13.58 (0.300s)\n",
      "  Simple Perplexity: 1.54 (0.007s)\n",
      "\n",
      "Séquence 7 (166 AA):\n",
      "  pLDDT: 67.33 (2.360s)\n",
      "  Perplexity: 7.51 (0.093s)\n",
      "  Simple Perplexity: 1.40 (0.006s)\n",
      "\n",
      "Séquence 8 (213 AA):\n",
      "  pLDDT: 84.05 (3.730s)\n",
      "  Perplexity: 7.21 (0.156s)\n",
      "  Simple Perplexity: 1.43 (0.007s)\n",
      "\n",
      "Séquence 9 (247 AA):\n",
      "  pLDDT: 80.83 (5.078s)\n",
      "  Perplexity: 8.42 (0.215s)\n",
      "  Simple Perplexity: 1.45 (0.007s)\n",
      "\n",
      "Séquence 10 (273 AA):\n",
      "  pLDDT: 79.37 (6.737s)\n",
      "  Perplexity: 12.17 (0.284s)\n",
      "  Simple Perplexity: 1.51 (0.007s)\n"
     ]
    }
   ],
   "source": [
    "# Test des fonctions calculate_plddt et calculate_perplexity\n",
    "import time\n",
    "\n",
    "# Séquences de test de tailles variables\n",
    "df_train = pd.read_csv(\"/home/arthur/projets/protein-generation/experiments/models/noised_dplm_simple/training_sequences.csv\")\n",
    "train = df_train[\"sequence\"].tolist()\n",
    "\n",
    "sequences = train[:10]  # Utiliser les 10 premières séquences pour le test\n",
    "\n",
    "# Charger les modèles\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "ppl_model, ppl_tokenizer = load_perplexity_model(\"facebook/esm2_t6_8M_UR50D\", device)\n",
    "fold_model, fold_tokenizer = load_folding_model(\"facebook/esmfold_v1\", device)\n",
    "\n",
    "# Tester les fonctions\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"\\nSéquence {i+1} ({len(seq)} AA):\")\n",
    "    \n",
    "    # Test pLDDT\n",
    "    start = time.time()\n",
    "    plddt_mean = calculate_plddt(seq, fold_model, fold_tokenizer, device)\n",
    "    plddt_time = time.time() - start\n",
    "    print(f\"  pLDDT: {plddt_mean:.2f} ({plddt_time:.3f}s)\")\n",
    "    \n",
    "    # Test Perplexity\n",
    "    start = time.time()\n",
    "    perplexity = calculate_perplexity(seq, ppl_model, ppl_tokenizer, device)\n",
    "    ppl_time = time.time() - start\n",
    "    print(f\"  Perplexity: {perplexity:.2f} ({ppl_time:.3f}s)\")\n",
    "    \n",
    "    # Test Perplexity\n",
    "    start = time.time()\n",
    "    simple_perplexity = calculate_perplexity_simple(seq, ppl_model, ppl_tokenizer, device)\n",
    "    ppl_time = time.time() - start\n",
    "    print(f\"  Simple Perplexity: {simple_perplexity:.2f} ({ppl_time:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b02a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Loading perplexity model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Perplexity model loaded\n",
      "Loading folding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esmfold_v1 were not used when initializing EsmForProteinFolding: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForProteinFolding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForProteinFolding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Folding model loaded\n",
      "\n",
      "Séquence 1 (100 AA):\n",
      "  pLDDT: 59.40 (1.043s)\n",
      "  Perplexity: 5.09 (0.033s)\n",
      "  Simple Perplexity: 1.47 (0.007s)\n",
      "\n",
      "Séquence 2 (100 AA):\n",
      "  pLDDT: 40.14 (1.049s)\n",
      "  Perplexity: 18.65 (0.033s)\n",
      "  Simple Perplexity: 1.67 (0.007s)\n",
      "\n",
      "Séquence 3 (100 AA):\n",
      "  pLDDT: 43.49 (1.029s)\n",
      "  Perplexity: 16.02 (0.033s)\n",
      "  Simple Perplexity: 1.56 (0.007s)\n",
      "\n",
      "Séquence 4 (100 AA):\n",
      "  pLDDT: 65.54 (1.044s)\n",
      "  Perplexity: 7.99 (0.033s)\n",
      "  Simple Perplexity: 1.49 (0.007s)\n",
      "\n",
      "Séquence 5 (100 AA):\n",
      "  pLDDT: 49.44 (1.046s)\n",
      "  Perplexity: 17.27 (0.033s)\n",
      "  Simple Perplexity: 1.64 (0.007s)\n",
      "\n",
      "Séquence 6 (100 AA):\n",
      "  pLDDT: 60.34 (1.048s)\n",
      "  Perplexity: 6.40 (0.033s)\n",
      "  Simple Perplexity: 1.56 (0.007s)\n",
      "\n",
      "Séquence 7 (100 AA):\n",
      "  pLDDT: 38.84 (1.035s)\n",
      "  Perplexity: 16.86 (0.033s)\n",
      "  Simple Perplexity: 1.62 (0.007s)\n",
      "\n",
      "Séquence 8 (100 AA):\n",
      "  pLDDT: 43.82 (1.020s)\n",
      "  Perplexity: 18.51 (0.033s)\n",
      "  Simple Perplexity: 1.62 (0.007s)\n",
      "\n",
      "Séquence 9 (100 AA):\n",
      "  pLDDT: 38.38 (1.049s)\n",
      "  Perplexity: 17.83 (0.033s)\n",
      "  Simple Perplexity: 1.61 (0.007s)\n",
      "\n",
      "Séquence 10 (100 AA):\n",
      "  pLDDT: 34.89 (1.065s)\n",
      "  Perplexity: 18.79 (0.033s)\n",
      "  Simple Perplexity: 1.65 (0.007s)\n"
     ]
    }
   ],
   "source": [
    "# Test des fonctions calculate_plddt et calculate_perplexity\n",
    "import time\n",
    "\n",
    "# Séquences de test de tailles variables\n",
    "df_train = pd.read_csv(\"/home/arthur/projets/protein-generation/experiments/models/noised_dplm_simple/training_sequences.csv\")\n",
    "train = df_train[\"sequence\"].tolist()\n",
    "\n",
    "df_gen_masked = pd.read_csv('/home/arthur/projets/protein-generation/experiments/models/masked_dplm_simple/generated_sequences.csv')\n",
    "demasked = df_gen_masked[\"sequence\"].tolist()\n",
    "\n",
    "\n",
    "sequences = demasked[:10]  # Utiliser les 10 premières séquences pour le test\n",
    "\n",
    "# Charger les modèles\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "ppl_model, ppl_tokenizer = load_perplexity_model(\"facebook/esm2_t6_8M_UR50D\", device)\n",
    "fold_model, fold_tokenizer = load_folding_model(\"facebook/esmfold_v1\", device)\n",
    "\n",
    "# Tester les fonctions\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"\\nSéquence {i+1} ({len(seq)} AA):\")\n",
    "    \n",
    "    # Test pLDDT\n",
    "    start = time.time()\n",
    "    plddt_mean, _ = calculate_plddt(seq, fold_model, fold_tokenizer, device)\n",
    "    plddt_time = time.time() - start\n",
    "    print(f\"  pLDDT: {plddt_mean:.2f} ({plddt_time:.3f}s)\")\n",
    "    \n",
    "    # Test Perplexity\n",
    "    start = time.time()\n",
    "    perplexity = calculate_perplexity(seq, ppl_model, ppl_tokenizer, device)\n",
    "    ppl_time = time.time() - start\n",
    "    print(f\"  Perplexity: {perplexity:.2f} ({ppl_time:.3f}s)\")\n",
    "    \n",
    "    \n",
    "    # Test Perplexity\n",
    "    start = time.time()\n",
    "    simple_perplexity = calculate_perplexity_simple(seq, ppl_model, ppl_tokenizer, device)\n",
    "    ppl_time = time.time() - start\n",
    "    print(f\"  Simple Perplexity: {simple_perplexity:.2f} ({ppl_time:.3f}s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
