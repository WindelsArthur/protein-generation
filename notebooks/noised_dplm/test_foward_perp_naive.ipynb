{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ccd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys, pathlib, os\n",
    "project_root = pathlib.Path.home() / \"projets\" / \"protein-generation\"\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from scripts.evaluation.evaluation_metrics import *\n",
    "from scripts.models.noised_dplm.vocabulary import ProteinVocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75815521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading perplexity model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Perplexity model loaded\n",
      "Loading folding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esmfold_v1 were not used when initializing EsmForProteinFolding: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForProteinFolding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForProteinFolding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Folding model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:36<00:00, 51.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random                        \n",
    "\n",
    "class MutationExperiment:\n",
    "    def __init__(self, csv_path, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.vocab = ProteinVocabulary()\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Séquences exactement de longueur 100, limitées aux 10 premières\n",
    "        self.sequences = [seq for seq in df['sequence'].tolist() if len(seq) == 100][:10]\n",
    "        \n",
    "        self.ppl_model, self.ppl_tokenizer   = load_perplexity_model(\"facebook/esm2_t6_8M_UR50D\", device)\n",
    "        self.fold_model, self.fold_tokenizer = load_folding_model   (\"facebook/esmfold_v1\",   device)\n",
    "    \n",
    "    def mutate_sequence(self, sequence, positions, use_mask=True):\n",
    "        \"\"\"\n",
    "        use_mask = True           → ESM avec tokens [MASK] (context_masked)\n",
    "        use_mask = False          → ESM sans masking (no_mask)\n",
    "        use_mask = \"random\"       → mutation aléatoire (uniforme) sans ESM\n",
    "        \"\"\"\n",
    "        if len(positions) == 0:\n",
    "            return sequence\n",
    "        \n",
    "        if use_mask == \"random\":\n",
    "            mutated = list(sequence)\n",
    "            for pos in positions:\n",
    "                original = mutated[pos]\n",
    "                choices = [aa for aa in self.vocab.ALPHABET if aa != original]\n",
    "                mutated[pos] = random.choice(choices)\n",
    "            return ''.join(mutated)\n",
    "        \n",
    "        if use_mask:                              # context_masked\n",
    "            masked_seq = list(sequence)\n",
    "            for pos in positions:\n",
    "                masked_seq[pos] = self.ppl_tokenizer.mask_token\n",
    "            input_seq = ''.join(masked_seq)\n",
    "        else:                                     # no_mask\n",
    "            input_seq = sequence\n",
    "        \n",
    "        # Prédictions ESM\n",
    "        inputs = self.ppl_tokenizer(input_seq, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.ppl_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Application des mutations (AA le moins probable)\n",
    "        mutated_seq = list(sequence)\n",
    "        for pos in positions:\n",
    "            esm_pos = pos + 1  # Décalage pour le token [CLS] en 0\n",
    "            aa_probs = {\n",
    "                aa: probs[0, esm_pos, self.ppl_tokenizer.convert_tokens_to_ids(aa)].item()\n",
    "                for aa in self.vocab.ALPHABET\n",
    "            }\n",
    "            mutated_seq[pos] = min(aa_probs, key=aa_probs.get)\n",
    "        \n",
    "        return ''.join(mutated_seq)\n",
    "    \n",
    "    def run_experiment(self, output_csv=\"results.csv\"):\n",
    "        results = []\n",
    "        mutation_counts = list(range(0, 101, 10))\n",
    "        method_configs = [\n",
    "            (\"context_masked\", True),\n",
    "            (\"no_mask\",       False),\n",
    "            (\"random\",        \"random\")         \n",
    "        ]\n",
    "        \n",
    "        for seq_idx, seq in enumerate(tqdm(self.sequences)):\n",
    "            for num_mut in mutation_counts:\n",
    "                positions = torch.randperm(len(seq))[:num_mut].cpu().tolist()\n",
    "                \n",
    "                for method, use_mask in method_configs:\n",
    "                    mutated_seq = self.mutate_sequence(seq, positions, use_mask)\n",
    "                    \n",
    "                    plddt, _ = calculate_plddt(\n",
    "                        mutated_seq, self.fold_model, self.fold_tokenizer, self.device\n",
    "                    )\n",
    "                    perplexity = calculate_perplexity(\n",
    "                        mutated_seq, self.ppl_model, self.ppl_tokenizer, self.device\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'sequence_id':    seq_idx,\n",
    "                        'num_mutations':  num_mut,\n",
    "                        'method':         method,\n",
    "                        'plddt':          plddt,\n",
    "                        'perplexity':     perplexity,\n",
    "                        'mutated_sequence': mutated_seq\n",
    "                    })\n",
    "        \n",
    "        pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "        \n",
    "torch.manual_seed(42)\n",
    "random.seed(42)                        \n",
    "\n",
    "experiment = MutationExperiment(\n",
    "    '/home/arthur/projets/protein-generation/data/seq_clean_L100.csv'\n",
    ")\n",
    "experiment.run_experiment('mutation_comparison_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed1eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYSE DES RÉSULTATS DE MUTATION ===\n",
      "\n",
      "1. MOYENNES PAR SÉQUENCE (moyennées sur tous les nombres de mutations)\n",
      "======================================================================\n",
      "             plddt_context_masked  plddt_no_mask  plddt_random  \\\n",
      "sequence_id                                                      \n",
      "0                          37.262         34.888        39.135   \n",
      "1                          36.526         35.668        38.371   \n",
      "2                          36.547         35.946        40.228   \n",
      "3                          37.544         37.853        42.719   \n",
      "4                          36.625         35.565        41.170   \n",
      "5                          34.413         34.204        40.650   \n",
      "6                          37.483         35.078        43.142   \n",
      "7                          36.595         34.842        42.099   \n",
      "8                          38.969         34.982        40.522   \n",
      "9                          35.292         32.666        39.453   \n",
      "\n",
      "             perplexity_context_masked  perplexity_no_mask  perplexity_random  \n",
      "sequence_id                                                                    \n",
      "0                               10.466              11.701             17.611  \n",
      "1                               10.797              13.367             17.937  \n",
      "2                                9.376              11.651             16.727  \n",
      "3                               10.569              13.133             17.362  \n",
      "4                                9.836              11.129             16.234  \n",
      "5                               12.005              13.176             18.598  \n",
      "6                               11.827              14.502             19.141  \n",
      "7                               11.486              12.822             18.587  \n",
      "8                               11.440              12.274             18.378  \n",
      "9                               11.896              12.069             19.933   \n",
      "\n",
      "2. MOYENNES PAR NOMBRE DE MUTATIONS (moyennées sur toutes les séquences)\n",
      "===========================================================================\n",
      "               plddt_context_masked  plddt_no_mask  plddt_random  \\\n",
      "num_mutations                                                      \n",
      "0                            63.756         63.756        63.756   \n",
      "10                           50.191         52.451        56.612   \n",
      "20                           33.785         36.165        50.150   \n",
      "30                           32.961         33.842        42.462   \n",
      "40                           30.389         29.299        35.140   \n",
      "50                           30.666         30.129        35.276   \n",
      "60                           42.039         28.853        33.526   \n",
      "70                           37.912         28.654        33.236   \n",
      "80                           31.037         27.727        31.606   \n",
      "90                           27.442         28.121        34.134   \n",
      "100                          23.805         27.867        32.339   \n",
      "\n",
      "               perplexity_context_masked  perplexity_no_mask  \\\n",
      "num_mutations                                                  \n",
      "0                                  8.056               8.056   \n",
      "10                                13.933              13.715   \n",
      "20                                18.091              17.100   \n",
      "30                                18.424              19.316   \n",
      "40                                17.439              17.942   \n",
      "50                                14.840              16.769   \n",
      "60                                11.224              13.704   \n",
      "70                                 7.959              12.308   \n",
      "80                                 6.166               8.680   \n",
      "90                                 3.297               6.751   \n",
      "100                                1.237               4.065   \n",
      "\n",
      "               perplexity_random  \n",
      "num_mutations                     \n",
      "0                          8.056  \n",
      "10                        10.405  \n",
      "20                        13.725  \n",
      "30                        16.685  \n",
      "40                        20.768  \n",
      "50                        20.212  \n",
      "60                        20.834  \n",
      "70                        21.480  \n",
      "80                        22.062  \n",
      "90                        22.415  \n",
      "100                       21.918   \n",
      "\n",
      "3. MOYENNES GLOBALES (moyennées sur séquences ET mutations)\n",
      "============================================================\n",
      "                 plddt  perplexity\n",
      "method                            \n",
      "context_masked  36.726      10.970\n",
      "no_mask         35.169      12.582\n",
      "random          40.749      18.051 \n",
      "\n",
      "4. DIFFÉRENCES ENTRE MÉTHODES\n",
      "===================================\n",
      "Différences (context_masked - no_mask) :\n",
      "  pLDDT     : 1.557\n",
      "  Perplexity: -1.612\n",
      "\n",
      "Différences (context_masked - random) :\n",
      "  pLDDT     : -4.023\n",
      "  Perplexity: -7.081\n",
      "\n",
      "Différences (no_mask - random) :\n",
      "  pLDDT     : -5.580\n",
      "  Perplexity: -5.469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  ANALYSE DES RÉSULTATS DE MUTATION\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger les résultats\n",
    "df = pd.read_csv('mutation_comparison_results.csv')\n",
    "\n",
    "print(\"=== ANALYSE DES RÉSULTATS DE MUTATION ===\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Tableau par séquence (moyenne sur tous les nombres de mutations)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"1. MOYENNES PAR SÉQUENCE (moyennées sur tous les nombres de mutations)\")\n",
    "print(\"=\" * 70)\n",
    "seq_analysis = (\n",
    "    df.groupby(['sequence_id', 'method'])\n",
    "      .agg(plddt=('plddt', 'mean'),\n",
    "           perplexity=('perplexity', 'mean'))\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "seq_pivot = seq_analysis.unstack('method')\n",
    "seq_pivot.columns = [f'{metric}_{method}' for metric, method in seq_pivot.columns]\n",
    "print(seq_pivot, \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Tableau par nombre de mutations (moyenne sur toutes les séquences)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"2. MOYENNES PAR NOMBRE DE MUTATIONS (moyennées sur toutes les séquences)\")\n",
    "print(\"=\" * 75)\n",
    "mut_analysis = (\n",
    "    df.groupby(['num_mutations', 'method'])\n",
    "      .agg(plddt=('plddt', 'mean'),\n",
    "           perplexity=('perplexity', 'mean'))\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "mut_pivot = mut_analysis.unstack('method')\n",
    "mut_pivot.columns = [f'{metric}_{method}' for metric, method in mut_pivot.columns]\n",
    "print(mut_pivot, \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Tableau global (moyenné sur séquences ET mutations)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"3. MOYENNES GLOBALES (moyennées sur séquences ET mutations)\")\n",
    "print(\"=\" * 60)\n",
    "global_analysis = (\n",
    "    df.groupby('method')\n",
    "      .agg(plddt=('plddt', 'mean'),\n",
    "           perplexity=('perplexity', 'mean'))\n",
    "      .round(3)\n",
    ")\n",
    "print(global_analysis, \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Différences entre méthodes\n",
    "# ------------------------------------------------------------------\n",
    "print(\"4. DIFFÉRENCES ENTRE MÉTHODES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def diff(a, b, metric):\n",
    "    return global_analysis.loc[a, metric] - global_analysis.loc[b, metric]\n",
    "\n",
    "pairs = [\n",
    "    ('context_masked', 'no_mask'),\n",
    "    ('context_masked', 'random'),\n",
    "    ('no_mask', 'random')\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(f\"Différences ({a} - {b}) :\")\n",
    "    print(f\"  pLDDT     : {diff(a, b, 'plddt'):.3f}\")\n",
    "    print(f\"  Perplexity: {diff(a, b, 'perplexity'):.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0922f",
   "metadata": {},
   "source": [
    "conclusion : c'ets mieux no_mask car plddt plus bas (donc confiance plus basse) et perplexité plus haute (surprise plus grande)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
