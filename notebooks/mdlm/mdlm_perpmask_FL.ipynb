{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8bcc01",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a406c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face transformers\n",
    "from transformers import EsmTokenizer, EsmForMaskedLM #type: ignore\n",
    "\n",
    "import sys, pathlib, os\n",
    "project_root = pathlib.Path.home() / \"projets\" / \"protein-generation\"\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from scripts.utils import *\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d453f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_perplexity_model(\n",
    "    ppl_model_name: str, \n",
    "    device: str = \"cuda\"\n",
    ") -> Tuple[EsmForMaskedLM, EsmTokenizer]:\n",
    "    print(\"Loading perplexity model...\")\n",
    "    \n",
    "    # Load tokenizer and model from Hugging Face\n",
    "    ppl_tokenizer = EsmTokenizer.from_pretrained(ppl_model_name)\n",
    "    ppl_model = EsmForMaskedLM.from_pretrained(ppl_model_name)\n",
    "    \n",
    "    # Set model to evaluation mode and move to specified device\n",
    "    ppl_model.eval()\n",
    "    ppl_model.to(device)\n",
    "    \n",
    "    print(\"✓ Perplexity model loaded\")\n",
    "    return ppl_model, ppl_tokenizer\n",
    "\n",
    "# Model names\n",
    "PPL_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # For perplexity calculation\n",
    "\n",
    "# Global variables to store models (avoiding reloading)\n",
    "ppl_model = None\n",
    "ppl_tokenizer = None\n",
    "\n",
    "# Load both models\n",
    "ppl_model, ppl_tokenizer = load_perplexity_model(ppl_model_name=PPL_MODEL_NAME, device=device)\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64073140",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13582e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### VOCABULARY\n",
    "\n",
    "class ProteinVocabularyMask:\n",
    "    def __init__(self):\n",
    "        self.ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        self.MASK_TOKEN = 20\n",
    "        self.VOCAB_SIZE = 21  # 20 AA + 1 MASK\n",
    "        self.vocab_to_id = {aa: i for i, aa in enumerate(self.ALPHABET)}\n",
    "        self.id_to_vocab = {i: aa for i, aa in enumerate(self.ALPHABET)}\n",
    "        self.id_to_vocab[self.MASK_TOKEN] = 'X'\n",
    "    \n",
    "    def encode_sequence(self, seq):\n",
    "        \"\"\"Encode une séquence d'acides aminés en tensor d'IDs.\"\"\"\n",
    "        return torch.tensor([self.vocab_to_id[aa] for aa in seq], dtype=torch.long)\n",
    "    \n",
    "    def decode_sequence(self, tokens):\n",
    "        \"\"\"Décode un tensor d'IDs en séquence d'acides aminés.\"\"\"\n",
    "        return ''.join([self.id_to_vocab[int(tok)] for tok in tokens])\n",
    "    \n",
    "    def encode_batch(self, sequences):\n",
    "        \"\"\"Encode un batch de séquences.\"\"\"\n",
    "        return torch.stack([self.encode_sequence(seq) for seq in sequences])\n",
    "\n",
    "\n",
    "### NOISE\n",
    "\n",
    "class NoiseSchedule:\n",
    "    def __init__(self, schedule_type='cosine'):\n",
    "        self.schedule_type = schedule_type\n",
    "    \n",
    "    def get_noise_level(self, t):\n",
    "        if self.schedule_type == 'linear':\n",
    "            return t\n",
    "        elif self.schedule_type == 'cosine':\n",
    "            return 1 - np.cos(t * np.pi / 2)\n",
    "        elif self.schedule_type == 'sqrt':\n",
    "            return np.sqrt(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x, t, noise_schedule, mask_token, ppl_model, ppl_tokenizer, vocab, device):\n",
    "    \"\"\"\n",
    "    Forward diffusion qui masque les positions qui augmentent le plus la perplexité.\n",
    "    \"\"\"\n",
    "    B, L = x.shape\n",
    "    \n",
    "    # TODO déterministe à changer\n",
    "    # Calculer le nombre de positions à masquer pour chaque séquence\n",
    "    num_masks = []\n",
    "    for ti in t:\n",
    "        mask_prob = noise_schedule.get_noise_level(float(ti))\n",
    "        # Binomiale parce que somme de bernouilli (vaiid mask avec prob mask_prob)\n",
    "        num_to_mask = torch.binomial(torch.tensor(L, dtype=torch.float), torch.tensor(mask_prob)).int().item()\n",
    "        num_masks.append(num_to_mask)\n",
    "    \n",
    "    # Initialiser les masques\n",
    "    mask = torch.zeros(B, L, dtype=torch.bool, device=x.device)\n",
    "    \n",
    "    # Pour chaque séquence, trouver les positions avec la plus haute perplexité\n",
    "    for b in range(B):\n",
    "        if num_masks[b] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Convertir la séquence en string\n",
    "        sequence_str = vocab.decode_sequence(x[b].cpu().numpy())\n",
    "        \n",
    "        # Calculer la perplexité pour chaque position\n",
    "        perplexities = []\n",
    "        for pos in range(L):\n",
    "            # Masquer cette position\n",
    "            masked_sequence = sequence_str[:pos] + ppl_tokenizer.mask_token + sequence_str[pos+1:]\n",
    "            \n",
    "            # Évaluer avec le modèle ESM\n",
    "            inputs = ppl_tokenizer(masked_sequence, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = ppl_model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                esm_pos = pos + 1  # Position après [CLS]\n",
    "                \n",
    "                # Probabilité du token original\n",
    "                original_aa = vocab.id_to_vocab[x[b, pos].item()]\n",
    "                original_token_id = ppl_tokenizer.convert_tokens_to_ids(original_aa)\n",
    "                original_prob = probs[0, esm_pos, original_token_id].item()\n",
    "                \n",
    "                # Perplexité = -log(prob)\n",
    "                perplexity = -torch.log(torch.tensor(original_prob + 1e-8)).item()\n",
    "                perplexities.append(perplexity)\n",
    "        \n",
    "        # Sélectionner les positions avec la plus haute perplexité\n",
    "        _, top_indices = torch.topk(torch.tensor(perplexities), num_masks[b])\n",
    "        mask[b, top_indices] = True\n",
    "    \n",
    "    # Appliquer les masques\n",
    "    xt = x.clone()\n",
    "    xt[mask] = mask_token\n",
    "    \n",
    "    return xt, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DENOISING TRANSFORMER\n",
    "\n",
    "class DenoisingTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_length, d_model=256, n_heads=8, n_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(seq_length, d_model)\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, n_heads, d_model * 4, \n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.output_head = nn.Linear(d_model, 20)  # Seulement les 20 AA\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        B, L = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        h = self.token_emb(x)\n",
    "        h += self.pos_emb(torch.arange(L, device=x.device)).unsqueeze(0)\n",
    "        h += self.time_emb(t.unsqueeze(1)).expand(-1, L, -1)\n",
    "        \n",
    "        # Transformer\n",
    "        h = self.transformer(h)\n",
    "        \n",
    "        # Sortie\n",
    "        return self.output_head(h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### INFERENCE\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoise_step(model, x, noise_schedule, t_current, t_next, mask_token):\n",
    "    B, L = x.shape\n",
    "    \n",
    "    noise_curr = noise_schedule.get_noise_level(t_current)\n",
    "    noise_next = noise_schedule.get_noise_level(t_next)\n",
    "    \n",
    "    if noise_curr > 1e-6:\n",
    "        reveal_prob = (noise_curr - noise_next) / noise_curr\n",
    "        # j'ai bien vérifié avec ma feuille de calculs prise en photo\n",
    "    else:\n",
    "        reveal_prob = 1.0\n",
    "    \n",
    "    # Prédictions du modèle\n",
    "    t_tensor = torch.full((B, 1), t_current, device=x.device)\n",
    "    logits = model(x, t_tensor)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Positions actuellement masquées\n",
    "    mask_pos = (x == mask_token)\n",
    "    \n",
    "    # Décider quelles positions révéler\n",
    "    reveal_mask = (torch.rand(B, L, device=x.device) < reveal_prob) & mask_pos\n",
    "    \n",
    "    # Échantillonner de nouveaux tokens\n",
    "    x_new = x.clone()\n",
    "    if reveal_mask.any():\n",
    "        samples = torch.multinomial(probs[reveal_mask], 1).squeeze(-1)\n",
    "        x_new[reveal_mask] = samples\n",
    "    \n",
    "    return x_new\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sequences(model, n_samples, seq_length, noise_schedule, dt, mask_token):\n",
    "    \"\"\"Génère des séquences par débruitage itératif.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.full((n_samples, seq_length), mask_token, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Débruitage itératif\n",
    "    t = 1.0\n",
    "    while t > 0:\n",
    "        t_next = max(t - dt, 0.0)\n",
    "        x = denoise_step(model, x, noise_schedule, t, t_next, mask_token)\n",
    "        t = t_next\n",
    "    \n",
    "    # Nettoyer les masques restants\n",
    "    if (x == mask_token).any():\n",
    "        t_tensor = torch.zeros((n_samples, 1), device=x.device)\n",
    "        logits = model(x, t_tensor)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        mask_pos = (x == mask_token)\n",
    "        if mask_pos.any():\n",
    "            samples = torch.multinomial(probs[mask_pos], 1).squeeze(-1)\n",
    "            x[mask_pos] = samples\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### LOSS AND TRAINING\n",
    "\n",
    "def compute_loss(model, x0, noise_schedule, mask_token, ppl_model, ppl_tokenizer, vocab, device):\n",
    "    \"\"\"Calcule la loss pour un batch.\"\"\"\n",
    "    B, L = x0.shape\n",
    "    \n",
    "    # Timesteps aléatoires\n",
    "    t = torch.rand(B, device=x0.device)\n",
    "    \n",
    "    # Forward diffusion\n",
    "    xt, mask = forward_diffusion(x0, t, noise_schedule, mask_token, ppl_model, ppl_tokenizer, vocab, device)\n",
    "    \n",
    "    # Prédictions du modèle\n",
    "    logits = model(xt, t.unsqueeze(1))\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=x0.device, requires_grad=True), 0.0\n",
    "    \n",
    "    # Loss seulement sur les positions masquées\n",
    "    loss = F.cross_entropy(logits[mask], x0[mask], reduction='mean')\n",
    "    mask_ratio = mask.sum().item() / mask.numel()\n",
    "    \n",
    "    return loss, mask_ratio\n",
    "\n",
    "\n",
    "def train_step(model, batch, optimizer, noise_schedule, mask_token, ppl_model, ppl_tokenizer, vocab, device):\n",
    "    \"\"\"Un pas d'entraînement.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    loss, mask_ratio = compute_loss(model, batch, noise_schedule, mask_token, ppl_model, ppl_tokenizer, vocab, device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), mask_ratio\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, optimizer, noise_schedule_fn, n_epochs, mask_token, ppl_model, ppl_tokenizer, vocab, device):\n",
    "    \"\"\"Boucle d'entraînement complète.\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch_data in dataloader:\n",
    "            batch = batch_data[0].to(next(model.parameters()).device)\n",
    "            loss, _ = train_step(model, batch, optimizer, noise_schedule_fn, mask_token, ppl_model, ppl_tokenizer, vocab, device)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72bee8",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb17aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Main experiment workflow\n",
    "print(\"=== STARTING EXPERIMENT ===\")\n",
    "config = load_experiment_config(\"/home/arthur/projets/protein-generation/configs/base_config.yaml\")\n",
    "config = setup_experiment_directory(config)\n",
    "\n",
    "print(f\"Experiment name: {config['experiment']['name']}\")\n",
    "print(f\"Experiment directory: {config['exp_dir']}\")\n",
    "\n",
    "# Immediately save the configuration\n",
    "save_experiment_config(config, config['exp_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device and data setup\n",
    "device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(config['training']['seed'])\n",
    "np.random.seed(config['training']['seed'])\n",
    "\n",
    "protein_data = pd.read_csv(config['data']['input_file'])\n",
    "sequences = protein_data['sequence'].tolist()\n",
    "sequences = sequences[:config['training']['n_samples']]\n",
    "print(f'{len(sequences)} séquences')\n",
    "print(set([len(seq) for seq in sequences]))\n",
    "\n",
    "\n",
    "# Model creation\n",
    "vocabulary = ProteinVocabularyMask()\n",
    "encoded = vocabulary.encode_batch(sequences)\n",
    "dataset = torch.utils.data.TensorDataset(encoded)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=config['training']['batch_size'], \n",
    "                                         shuffle=True)\n",
    "\n",
    "model = DenoisingTransformer(\n",
    "    vocab_size=vocabulary.VOCAB_SIZE,\n",
    "    seq_length=config['model']['seq_length'],\n",
    "    d_model=config['model']['d_model'],\n",
    "    n_heads=config['model']['n_heads'],\n",
    "    n_layers=config['model']['n_layers'],\n",
    "    dropout=config['model']['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "    \n",
    "# Noise schedule function\n",
    "noise_schedule = NoiseSchedule(config['diffusion']['noise_schedule'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TRAINING ===\")\n",
    "losses = train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,                                                                \n",
    "    noise_schedule_fn=noise_schedule,\n",
    "    mask_token=vocabulary.MASK_TOKEN,\n",
    "    n_epochs=config['training']['n_epochs']\n",
    ")\n",
    "\n",
    "# Plot losses\n",
    "plot_and_save_losses(config, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence generation\n",
    "print(\"\\n=== GENERATION ===\")\n",
    "generated_tokens = generate_sequences(\n",
    "    model=model,\n",
    "    n_samples=config['generation']['n_samples'],\n",
    "    seq_length=config['model']['seq_length'],\n",
    "    noise_schedule=noise_schedule,\n",
    "    dt=config['generation']['dt'],\n",
    "    mask_token=vocabulary.MASK_TOKEN\n",
    ")\n",
    "\n",
    "# Decode sequences\n",
    "generated_sequences = [vocabulary.decode_sequence(seq) for seq in generated_tokens]\n",
    "\n",
    "# Display and save results\n",
    "display_sample_sequences(config, generated_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== SAVING RESULTS ===\")\n",
    "save_results(config, model, losses, generated_sequences)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n=== EXPERIMENT COMPLETE ===\")\n",
    "print(f\"Name: {config['experiment']['name']}\")\n",
    "print(f\"Directory: {config['exp_dir']}\")\n",
    "print(f\"Training sequences: {len(sequences)}\")\n",
    "print(f\"Epochs: {config['training']['n_epochs']}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Generated sequences: {len(generated_sequences)}\")\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "for fname in os.listdir(config['exp_dir']):\n",
    "    print(f\"  - {fname}\")\n",
    "\n",
    "print(\"\\nExperiment finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
